# 1. 网络IO的发展

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632312156.jpg" alt="img" style="zoom:75%;" />

## 1.1 网络 IO 的各个发展阶段

通常，我们在此讨论的网络 IO 一般都是针对 linux 操作系统而言。网络 IO 的发展过程是随着 linux 的内核演变而变化，因此网络 IO 大致可以分为如下几个阶段：

1. 阻塞 IO(BIO)
2. 非阻塞 IO(NIO)
3. IO 多路复用第一版(select/poll)
4. IO 多路复用第二版(epoll)
5. 异步 IO(AIO)

而每一个阶段，都是因为当前的网络有一些缺陷，因此又在不断改进该缺陷。这是**网络 IO 一直演变过程中的本质**。下面将对上述几个阶段进行介绍，并对每个阶段的网络 IO 解决了哪些问题、优点、缺点进行剖析。

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632315587.jpg" alt="preview" style="zoom:50%;" />

## 1.2 网络的两个阶段

所谓的**I/O 就是计算机内存与外部设备之间拷贝数据的过程**。我们知道 CPU 访问内存的速度远远高于外部设备，因此 CPU 是先把外部设备的数据读到内存里，然后再进行处理。请考虑一下这个场景，当你的程序通过 CPU 向外部设备发出一个读指令时，数据从外部设备拷贝到内存往往需要一段时间，这个时候 CPU 没事干了，你的程序是主动把 CPU 让给别人？还是让 CPU 不停地查：数据到了吗，数据到了吗……

这就是 I/O 模型要解决的问题。

在网络中，我们通常可以将其广义上划分为以下两个阶段：

**第一阶段：硬件接口到内核态**

**第二阶段：内核态到用户态**

本人理解：我们通常上网，大部分数据都是通过网线传递的。因此对于两台计算机而言，要进行网络通信，其数据都是先从应用程序传递到传输层(TCP/UDP)到达内核态，然后再到网络层、数据链路层、物理层，接着数据传递到硬件网卡，最后通过网络传输介质传递到对端机器的网卡，然后再一步一步数据从网卡传递到内核态，最后再拷贝到用户态。

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632315946.png" alt="image-20210922210546885" style="zoom:50%;" />

针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。

对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：

1. 数据准备阶段：等待数据准备 (Waiting for the data to be ready)
2. 数据拷贝阶段：将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)

## 1.3 阻塞 IO 和非阻塞 IO 的区别

根据 1.2 节的内容，我们可以知道，网络中的数据传输从网络传输介质到达目的机器，需要如上两个阶段。此处我们把从**硬件到内核态**这一阶段，是否发生阻塞等待，可以将网络分为**阻塞 IO**和**非阻塞 IO**。如果用户发起了读写请求，但内核态数据还未准备就绪，该阶段不会阻塞用户操作，内核立马返回，则称为非阻塞 IO。如果该阶段一直阻塞用户操作。直到内核态数据准备就绪，才返回。这种方式称为阻塞 IO。

因此，区分阻塞 IO 和非阻塞 IO 主要看第一阶段是否阻塞用户操作。

## 1.4 同步 IO 和异步 IO 的区别

从前面我们知道了，数据的传递需要两个阶段，在此处只要任何一个阶段会阻塞用户请求，都将其称为同步 IO，两个阶段都不阻塞，则称为异步 IO。

在目前所有的操作系统中，linux 中的 epoll、mac 的 kqueue 都属于同步 IO，因为其在第二阶段(数据从内核态到用户态)都会发生拷贝阻塞。 而只有 windows 中的 IOCP 才真正属于异步 IO，即 AIO。



# 2. 网络IO模型

正式因为有数据准备和数据拷贝这两个阶段，linux系统产生了下面五种网络模式的方案。

- 阻塞 I/O（blocking IO）
- 非阻塞 I/O（nonblocking IO）
- I/O 多路复用（ IO multiplexing）
- 信号驱动 I/O（ signal driven IO）
- 异步 I/O（asynchronous IO）

注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。

## 2.1 阻塞 IO（blocking IO）

在本节，我们将介绍最初的阻塞 IO，阻塞 IO 英文为 blocking IO，又称为 BIO。根据前面的介绍，阻塞 IO 主要指的是第一阶段(硬件网卡到内核态)。

阻塞 IO，顾名思义当用户发生了系统调用后，如果数据未从网卡到达内核态，内核态数据未准备好，此时会一直阻塞。直到数据就绪，然后从内核态拷贝到用户态再返回。

![clipboard.png](http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632324655.png)

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632324730.png" alt="image-20210922233210931" style="zoom:50%;" />

上图中，应用进程通过系统调用 recvfrom 接收数据，但由于内核还未准备好数据报，应用进程就阻塞住了（当然，是进程自己选择的阻塞）。直到内核准备好数据报，recvfrom 完成数据报复制工作，kernel返回结果，应用进程才结束阻塞状态，重新运行起来。

> recvfrom 一般用于 UDP 协议中，但是如果在 TCP 中 connect 函数调用后也可以用。用于从（已连接）套接口上接收数据，并捕获数据发送源的地址。也就是我们本文中以及书中说的真正的 I/O 操作。

这里简单解释一下应用进程和内核的关系。内核即操作系统内核，用于控制计算机硬件。同时将用户态的程序和底层硬件隔离开，以保障整个计算机系统的稳定运转（如果用户态的程序可以控制底层硬件，那么一些病毒就会针对硬件进行破坏，比如 CIH 病毒）。应用进程即用户态进程，运行于操作系统之上，通过系统调用与操作系统进行交互。上图中，内核指的是 TCP/IP 等协议及相关驱动程序。客户端发送的请求，并不是直接送达给应用程序，而是要先经过内核。内核将请求数据缓存在内核空间，应用进程通过 recvfrom 调用，将数据从内核空间拷贝到自己的进程空间内。

> 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。

在一般使用阻塞 IO 时，都需要配置多线程来使用，最常见的模型是**阻塞 IO+多线层**，每个连接一个单独的线程进行处理。

**我们知道，一般一个程序可以开辟的线程是优先的，而且开辟线程的开销也是比较大的。也正是这种方式，会导致一个应用程序可以处理的客户端请求受限。面对百万连接的情况，是无法处理。**

既然发现了问题，分析了问题，那就得解决问题。既然阻塞 IO 有问题，本质是由于其阻塞导致的，因此自然而然引出了下面即将介绍的主角：**非阻塞 IO**

## 2.2 非阻塞IO（nonblocking IO）

与阻塞 IO 模型相反，在非阻塞 IO 模型下应用进程与内核交互，就是在第一阶段(网卡-内核态)数据未到达时，不再一味的等着，而是直接返回。然后通过轮询的方式，需要不停的去问内核数据准备好没。在等待数据从内核空间拷贝到用户空间这段时间里，线程还是阻塞的，等数据到了用户空间再把线程叫醒。

![img](http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632325211.jpg)

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632325232.png" alt="image-20210922234032331" style="zoom:50%;" />

非阻塞 IO 是需要系统内核支持的，在创建了连接后，可以调用 setsockop 设置 noblocking。应用进程通过 recvfrom 系统调用不停的去和内核交互，直到内核准备好数据报。从上面的流程中可以看出，应用进程进入轮询状态时等同于阻塞，所以非阻塞的 I/O 似乎并没有提高进程工作效率。

> 所以，nonblocking IO的特点是用户进程需要**不断的主动询问**kernel数据好了没有。

正如前面提到的，非阻塞 IO 解决了阻塞 IO**每个连接一个线程处理的问题**，所以其最大的优点就是 **一个线程可以处理多个连接**，这也是其非阻塞决定的。

但这种模式，也有一个问题，就是需要用户多次发起系统调用。**频繁的系统调用**是比较消耗系统资源的。

因此，既然存在这样的问题，那么自然而然我们就需要解决该问题：**保留非阻塞 IO 的优点的前提下，减少系统调用**

## 2.3 IO多路复用第一版

为了解决非阻塞 IO 存在的频繁的系统调用这个问题，随着内核的发展，出现了 IO 多路复用模型。那么我们就需要搞懂几个问题：

- IO 多路复用到底复用什么？
- IO 多路复用如何复用？

**IO 多路复用：** 很多人都说，IO 多路复用是用一个线程来管理多个网络连接，但本人不太认可，因为在非阻塞 IO 时，就已经可以实现一个线程处理多个网络连接了，这个是由于其非阻塞而决定的。

**在此处，个人观点，多路复用主要复用的是通过有限次的系统调用来实现管理多个网络连接。最简单来说，我目前有 10 个连接，我可以通过一次系统调用将这 10 个连接都丢给内核，让内核告诉我，哪些连接上面数据准备好了，然后我再去读取每个就绪的连接上的数据。因此，IO 多路复用，复用的是系统调用。通过有限次系统调用判断海量连接是否数据准备好了。**

用户线程的读取操作分成两步了，线程先发起 select 调用，目的是问内核数据准备好了吗？等内核把数据准备好了，用户线程再发起 read 调用。在等待数据从内核空间拷贝到用户空间这段时间里，线程还是阻塞的。

那为什么叫 I/O 多路复用呢？因为一次 select 调用可以向内核查多个数据通道（Channel）的状态，所以叫多路复用。

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202201/14/1642129867.jpeg" alt="16" style="zoom:50%;" />

**无论下面的 select、poll、epoll，其都是这种思想实现的，不过在实现上，select/poll 可以看做是第一版，而 epoll 是第二版。**

**IO 多路复用第一版，这个概念是本人想出来的，主要是方便将 select/poll 和 epoll 进行区分。**

所以此处 IO 多路复用第一版，主要特指 select 和 poll 这两个。

本节接下来将以 select 函数为例，简述该函数的使用过程。

select 有三个文件描述符集，分别是可读文件描述符集（readfds）、可写文件描述符集（writefds）和异常文件描述符集（exceptfds）。应用程序可将某个 socket （文件描述符）设置到感兴趣的文件描述符集中，并调用 select 等待所感兴趣的事件发生。比如某个 socket 处于可读状态了，此时应用进程就可调用 recvfrom 函数把数据从内核空间拷贝到进程空间内，无需再等待内核准备数据了。

![img](http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632325638.jpg)

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/22/1632325694.png" alt="image-20210922234814031" style="zoom:50%;" />

当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

> 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。

这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。

所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过**process是被select这个函数block，而不是被socket IO给block**。

**从本质来说：IO 多路复用中，select()/poll()/epoll_wait()这几个函数对应第一阶段；read()/recvfrom()对应第二阶段。**



**select 和 poll 的区别**

1. select 能处理的最大连接，默认是 1024 个，可以通过修改配置来改变，但终究是有限个；而 poll 理论上可以支持无限个
2. select 和 poll 在管理海量的连接时，会频繁的从用户态拷贝到内核态，比较消耗资源。



**IO 多路复用，主要在于复用，通过 select()或者 poll()将多个 socket fds 批量通过系统调用传递给内核，由内核进行循环遍历判断哪些 fd 上数据就绪了，然后将就绪的 readyfds 返回给用户。再由用户进行挨个遍历就绪好的 fd，读取或者写入数据。**

**所以通过 IO 多路复用+非阻塞 IO，一方面降低了系统调用次数，另一方面可以用极少的线程来处理多个网络连接。**

虽然第一版 IO 多路复用解决了之前提到的频繁的系统调用次数，但同时引入了新的问题：**用户需要每次将海量的 socket fds 集合从用户态传递到内核态，让内核态去检测哪些网络连接数据就绪了。**

**但这个地方会出现频繁的将海量 fd 集合从用户态传递到内核态，再从内核态拷贝到用户态。 所以，这个地方开销也挺大。**

既然还有这个问题，那我们继续开始解决这个问题，因此就引出了第二版的 IO 多路复用。

**其实思路也挺简单，既然需要拷贝，那就想办法，不拷贝。既然不拷贝，那就在内核开辟一段区域咯。**

## 2.4 IO多路复用第二版

IO 多路复用第二版主要指 epoll，epoll 的出现也是随着内核版本迭代才诞生的，在网上到处看到，epoll 是内核 2.6 以后开始支持的。

**epoll 的出现是为了解决前面提到的 IO 多路复用第一版的问题。**

```c
// 创建epollFd，底层是在内核态分配一段区域，底层数据结构红黑树+双向链表
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大

// 往红黑树中增加、删除、更新管理的socket fd
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；

// 这个api是用来在第一阶段阻塞，等待就绪的fd。
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

![img](http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632326942.jpg)

![img](http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632326970.png)

当 epoll_wait()调用后会阻塞，然后完了当返回时，会返回了哪些 fd 的数据就绪了，用户只需要遍历就绪的 fd 进行读写即可。

**IO 多路复用第二版 epoll 的优点在于：**

一开始就在内核态分配了一段空间，来存放管理的 fd,所以在每次连接建立后，交给 epoll 管理时，需要将其添加到原先分配的空间中，后面再管理时就不需要频繁的从用户态拷贝管理的 fd 集合。通通过这种方式大大的提升了性能。

所以现在的 IO 多路复用主要指 epoll。

## 2.5 信号驱动式 IO 模型

信号驱动式 IO 模型是指，应用进程告诉内核，如果某个 socket 的某个事件发生时，请向我发一个信号。在收到信号后，信号对应的处理函数会进行后续处理。

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632327148.png" alt="image-20210923001228683" style="zoom:50%;" />

从流程上来说，这种方式确实更合理。进程在信号到来之前，可以去做其他事情，而不用忙等。但现实中，这种 I/O 模型用的并不多。

## 2.6 异步 IO

前面介绍的所有网络 IO 都是同步 IO，因为当数据在内核态就绪时，在内核态拷贝用用户态的过程中，仍然会有短暂时间的阻塞等待。而异步 IO 指：**内核态拷贝数据到用户态这种方式也是交给系统线程来实现，不由用户线程完成**，目前只有 windows 系统的 IOCP 是属于异步 IO。

![img](http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632327250.jpg)

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632327278.png" alt="image-20210923001438560" style="zoom:50%;" />

通过 aio_read 把文件描述符、数据缓存空间，以及信号告诉内核，当文件描述符处于可读状态时，内核会亲自将数据从内核空间拷贝到应用进程指定的缓存空间。拷贝完在告诉进程 I/O 操作结束，你可以直接使用数据了。

用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。

## 2.7 总结

### blocking和non-blocking的区别

调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。

### synchronous IO和asynchronous IO的区别

在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：

- A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;
- An asynchronous I/O operation does not cause the requesting process to be blocked;

两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。

有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。

而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。

**各个IO Model的比较如图所示：**

![clipboard.png](http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632327987.png)

<img src="http://blog-1259650185.cosbj.myqcloud.com/img/202109/23/1632328037.png" alt="image-20210923002717785" style="zoom:50%;" />

通过上面的图片，可以看出阻塞式 IO、非阻塞式 IO、IO 多路复用、信号驱动式 IO 他们的第二阶段都相同，也就是都会阻塞到 recvfrom 调用（就是图中“发起”的动作）上。

阻塞式 IO 和 IO 多路复用，两个阶段都阻塞，那区别在哪里呢？虽然IO 多路复用第一阶段都是阻塞，但是阻塞式 IO 如果要接收更多的连接，就**必须创建更多的线程**。IO 多路复用模式下在第一个阶段大量的连接统统都可以过来直接注册到 Selector 复用器上面，同时只要单个或者少量的线程来循环处理这些连接事件就可以了，一旦达到“就绪”的条件，就可以立即执行真正的 IO 操作。这就是 IO 复用与传统的阻塞式 IO 最大的不同。也正是 IO 复用的精髓所在。

从应用进程的角度去理解始终是阻塞的，等待数据和将数据复制到用户进程这两个阶段都是阻塞的。这一点我们从应用程序是可以清楚的得知，比如我们调用一个以 IO 多路复用为基础的 NIO 应用服务。调用端是一直阻塞等待返回结果的。

从内核的角度等待 Selector 上面的网络事件就绪，是阻塞的，如果没有任何一个网络事件就绪则一直等待直到有一个或者多个网络事件就绪。但是从内核的角度考虑，有一点是不阻塞的，就是复制数据，因为内核不用等待，当有就绪条件满足的时候，它直接复制，其余时间在处理别的就绪的条件。这也是大家一直说的非阻塞 IO。实际上是就是指的这个地方的非阻塞（不阻塞IO，只阻塞select函数）。

当我们阅读《UNIX网络编程》（第三版）一书的时候。P124，6.2.3 小节中“而不是阻塞在真正的 I/O 系统调用上”这里的阻塞是相对内核来说的。P127，6.2.7 小节“因为其中真正的 I/O 操作（recvfrom）将阻塞进程”这里的阻塞是相对用户进程来说的。明白了这两点，理解起来就不矛盾了，而且一通到底！

而non-blocking IO和asynchronous IO的区别也是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。



# 参考

[I/O模型简述](https://www.tianxiaobo.com/2018/02/08/IO%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%BF%B0/)

[Linux IO模式及 select、poll、epoll详解](https://segmentfault.com/a/1190000003063859)

[网络 IO 演变过程](https://zhuanlan.zhihu.com/p/353692786)

[解读 I/O 多路复用技术](http://www.linkedkeeper.com/1361.html)

